{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled1.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyP8VXohJwRXIfoyrFkAM1dJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/stefanopieri/machine_learning_module/blob/main/ml.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "hakgFWaIlurb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install keras\n",
        "!pip3 install tensorflow\n",
        "!pip install random\n",
        "!pip3 install opencv-python\n",
        "!pip3 install numpy\n",
        "!pip3 install pandas\n",
        "!pip3 install sklearn\n",
        "!pip3 install matplotlib\n",
        "!pip3 install os\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Doaxhy7JmHJ2",
        "outputId": "2ccf0ad7-d72e-468e-8e63-b2d7d8d0f3ea"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.14.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: keras<2.9,>=2.8.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: gast>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: tf-estimator-nightly==2.8.0.dev2021122109 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0.dev2021122109)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n",
            "Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (13.0.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.21.5)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.24.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from tensorflow) (57.4.0)\n",
            "Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.44.0)\n",
            "Requirement already satisfied: tensorboard<2.9,>=2.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.8.0)\n",
            "Requirement already satisfied: flatbuffers>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.7/dist-packages (from astunparse>=1.6.0->tensorflow) (0.37.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (2.23.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.9,>=2.8->tensorflow) (0.6.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.9,>=2.8->tensorflow) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.9,>=2.8->tensorflow) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.9,>=2.8->tensorflow) (1.24.3)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.9,>=2.8->tensorflow) (3.2.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement random (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for random\u001b[0m\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/dist-packages (4.1.2.30)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.21.5)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (3.1.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.4.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib) (3.10.0.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement os (from versions: none)\u001b[0m\n",
            "\u001b[31mERROR: No matching distribution found for os\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from keras.datasets import mnist\n",
        "from keras.layers.merge import _Merge\n",
        "from keras.layers import Input, Dense, Reshape, Flatten, Dropout, multiply\n",
        "from keras.layers import BatchNormalization, Activation, ZeroPadding2D, Embedding\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
        "from keras.models import Sequential, Model\n",
        "from keras.metrics import Precision, Recall, AUC\n",
        "from tensorflow.keras.optimizers import RMSprop, Adam\n",
        "from functools import partial\n",
        "import cv2\n",
        "import tensorflow.keras.backend as K\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import random\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import applications\n",
        "from tensorflow.keras.layers import RandomFlip, RandomRotation, RandomTranslation\n",
        "from tensorflow.keras.applications import efficientnet_v2, efficientnet\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
        "from tensorflow.keras.applications import EfficientNetV2L\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.utils import image_dataset_from_directory\n",
        "from tensorflow.keras.applications.efficientnet_v2 import preprocess_input, decode_predictions, EfficientNetV2S\n",
        "import os\n",
        "import keras\n",
        "\n"
      ],
      "metadata": {
        "id": "kjjmcV3QmIIW"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def classical_augmentation(INPUT_FOLDER_ADDRESS, OUTPUT_FOLDER_ADDRESS):\n",
        "    classical_augm_net = Sequential([RandomFlip(\"horizontal\"),\n",
        "                                     RandomRotation(factor=(-0.05, 0.05)),\n",
        "                                     RandomTranslation(height_factor=(-0.15, 0.15), width_factor=(-0.15, 0.15),\n",
        "                                                       fill_mode='constant'), ])\n",
        "\n",
        "    # take ls of files in the input folder\n",
        "    ls = os.listdir(INPUT_FOLDER_ADDRESS)\n",
        "    #print(ls)\n",
        "    # loop across the ls of files\n",
        "    for file in ls:\n",
        "        #print(str(file)[-4:])\n",
        "        if str(file)[-4:] == '.png':\n",
        "            print(file[:-4])\n",
        "\n",
        "\n",
        "            #   import image w cv2\n",
        "            image = cv2.imread(str(INPUT_FOLDER_ADDRESS+'/'+str(file)))\n",
        "\n",
        "            for ix in range(0, 10):\n",
        "                img = classical_augm_net(image, training=True)\n",
        "                img = img.numpy()\n",
        "                print(type(img))\n",
        "                print(img.shape)\n",
        "                # plt.savefig(img, OUTPUT_FOLDER_ADDRESS + '/' + pic_name[:-4] + '_' + str(ix))\n",
        "                cv2.imwrite(OUTPUT_FOLDER_ADDRESS + '/' + str(file)[:-4] + '_' + str(ix)+'.png', img)\n",
        "\n",
        "\n",
        "\n",
        "def calculate_labels_for_classical_augm_data(ADDRESS_OLD_DATAFRAME):\n",
        "    # take ls of files in the input folder\n",
        "    data = []\n",
        "    df = pd.read_csv(ADDRESS_OLD_DATAFRAME, header=0, index_col=0)\n",
        "    df = df[['ix', 'mask_condition']]\n",
        "    ls_ix = df['ix'].values.astype('str')\n",
        "\n",
        "    #print(ls_ix)\n",
        "\n",
        "    for name in ls_ix:\n",
        "        for i in range(0,10):\n",
        "            print([name+'_'+str(i), df[df['ix'].astype('str')==name]['mask_condition'].values[0]])\n",
        "            data.append([name+'_'+str(i), df[df['ix'].astype('str')==name]['mask_condition'].values[0]])\n",
        "\n",
        "    #print(data)\n",
        "    df2 = pd.DataFrame(data, columns=['ix', 'mask_condition'])\n",
        "    df2['mask_condition'] = df2['mask_condition'].apply(lambda row : 0 if str(row)=='with_mask' else (1 if str(row)=='mask_weared_incorrect' else 2))\n",
        "    print(df2)\n",
        "    df2.to_csv(path_or_buf='/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv')\n",
        "\n",
        "    #ls = os.listdir(INPUT_FOLDER_ADDRESS)\n",
        "\n",
        "\n",
        "def replace_incorrect_names(x):\n",
        "    if str(x[-3:]) == 'png':\n",
        "\n",
        "        return x\n",
        "    else:\n",
        "        #print(x[-3:])\n",
        "        return '0_0_0.png'\n",
        "\n",
        "\n",
        "def replace_incorrect_names_val(x):\n",
        "    if str(x[-3:]) == 'png':\n",
        "        return x\n",
        "    else:\n",
        "        pass\n",
        "        #print(x[-3:])\n",
        "        #return '0_0_0.png'\n",
        "\n",
        "\n",
        "def repl_incorrect_names_from_list(ls):\n",
        "    out_ls=[]\n",
        "    for x in ls:\n",
        "        if str(x[-3:]) == 'png':\n",
        "            out_ls= out_ls.append(str(x))\n",
        "        else:\n",
        "            pass\n",
        "    print(out_ls)\n",
        "    return out_ls\n",
        "\n",
        "\n",
        "def weighted_categorical_crossentropy(y_true, y_pred, weights):\n",
        "    nb_cl = len(weights)\n",
        "    final_mask = K.zeros_like(y_pred[:, 0])\n",
        "    y_pred_max = K.max(y_pred, axis=1)\n",
        "    y_pred_max = K.reshape(y_pred_max, (K.shape(y_pred)[0], 1))\n",
        "    y_pred_max_mat = K.cast(K.equal(y_pred, y_pred_max), K.floatx())\n",
        "    for c_p, c_t in np.product(range(nb_cl), range(nb_cl)):\n",
        "        final_mask += (weights[c_t, c_p] * y_pred_max_mat[:, c_p] * y_true[:, c_t])\n",
        "\n",
        "    return K.categorical_crossentropy(y_pred, y_true) * final_mask\n",
        "\n",
        "\n",
        "def generate_weight_matrix():\n",
        "    w = np.zeros((3, 3))\n",
        "    w[0, 0] = 25.5/100\n",
        "    w[0, 1] = 25.5/100\n",
        "    w[0, 2] = 25.5/100\n",
        "    w[1, 0] = 2/100\n",
        "    w[1, 1] = 2/100\n",
        "    w[1, 2] = 2/100\n",
        "    w[2, 0] = 6/100\n",
        "    w[2, 1] = 6/100\n",
        "    w[2, 2] = 6/100\n",
        "    return w\n",
        "\n",
        "def generate_training_batch(batch_size, PATH_IMAGES = '/Volumes/seagate_expans/classical_augm_images/images',\n",
        "                            PATH_LABELS = '/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv'\n",
        "                             ):\n",
        "    ls = []\n",
        "    ls_files = os.listdir(PATH_IMAGES)\n",
        "\n",
        "    sample_images = random.sample(ls_files, batch_size)\n",
        "    sample_images = list(map(replace_incorrect_names, sample_images))\n",
        "    print(sample_images)\n",
        "    # print(sample_images)\n",
        "    for img_name in sample_images:\n",
        "        #print(img_name[:-4])\n",
        "        #print(PATH_IMAGES + '/'+str(img_name)+'.png')\n",
        "        img=cv2.imread(PATH_IMAGES + '/'+str(img_name))\n",
        "        ls.append(img)\n",
        "        #print(ls)\n",
        "\n",
        "    df = pd.read_csv(PATH_LABELS, header=0, index_col=0)\n",
        "\n",
        "\n",
        "    sample_images = [element[:-4] for element in sample_images]\n",
        "\n",
        "    y = df[df.ix.isin(sample_images)]['mask_condition'].to_numpy()\n",
        "\n",
        "    #print(y)\n",
        "    print(np.array(ls, dtype='float64').shape)\n",
        "    print(y.shape)\n",
        "    print(y)\n",
        "    X = np.array(ls, dtype='float64').reshape((batch_size, 80, 80, 3))\n",
        "    return X, y\n",
        "\n",
        "    #print(sample_images)\n",
        "        #ls.append\n",
        "\n",
        "def get_img_names_list(PATH_IMAGES):\n",
        "\n",
        "    ls_files = os.listdir(PATH_IMAGES)\n",
        "    for element in ls_files:\n",
        "        if element.startswith('.'):\n",
        "            ls_files.remove(element)\n",
        "        else:\n",
        "            pass\n",
        "    return ls_files\n",
        "\n",
        "\n",
        "def generate_training_data(batch_size, PATH_IMAGES = '/Volumes/seagate_expans/classical_augm_images/images',\n",
        "                            PATH_LABELS = '/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv'\n",
        "                             ):\n",
        "    ls = []\n",
        "\n",
        "\n",
        "\n",
        "    #ls_files = os.listdir(PATH_IMAGES)\n",
        "\n",
        "    ls_files = get_img_names_list(PATH_IMAGES)\n",
        "\n",
        "    sample_images = random.sample(ls_files, batch_size)\n",
        "\n",
        "    #if '.DS_store' in sample_images:\n",
        "    #    sample_images=sample_images.remove('.DS_store')\n",
        "\n",
        "    sample_images = list(map(replace_incorrect_names, sample_images))\n",
        "#    sample_images = repl_incorrect_names_from_list(sample_images)\n",
        "    # print(sample_images)\n",
        "    # print(sample_images)\n",
        "    for img_name in sample_images:\n",
        "        #print(img_name[:-4])\n",
        "        #print(PATH_IMAGES + '/'+str(img_name)+'.png')\n",
        "        img=cv2.imread(PATH_IMAGES + '/'+str(img_name))\n",
        "        ls.append(img)\n",
        "        #print(ls)\n",
        "\n",
        "    df = pd.read_csv(PATH_LABELS, header=0, index_col=0)\n",
        "\n",
        "\n",
        "    sample_images = [element[:-4] for element in sample_images]\n",
        "\n",
        "    y = df[df.ix.isin(sample_images)]['mask_condition'].to_numpy()\n",
        "\n",
        "    #print(y)\n",
        "    #print(len(sample_images))\n",
        "    #annot=pd.read_csv('/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv', header=0)['ix'].to_numpy().tolist()\n",
        "    #print(sample_images)\n",
        "    #quit()\n",
        "    #print('printing annot:')\n",
        "    #print(annot)\n",
        "    #flat_list = [str(item) for sublist in annot for item in sublist]\n",
        "\n",
        "    #print(set(sample_images).difference(set(flat_list)))\n",
        "    #quit()\n",
        "    #print(df[~df.ix.isin(sample_images)].to_numpy())\n",
        "    #quit()\n",
        "    #print(np.array(ls, dtype='float64').shape)\n",
        "    print(y.shape)\n",
        "    print(y)\n",
        "    X = np.array(ls, dtype='float64').reshape((batch_size, 80, 80, 3))\n",
        "    return X, y\n",
        "\n",
        "    #print(sample_images)\n",
        "        #ls.append\n",
        "\n",
        "\n",
        "\n",
        "def generate_validation_batch(batch_size, PATH_IMAGES = '/Volumes/seagate_expans/val_classical_augm_images/images',\n",
        "                            PATH_LABELS = '/Volumes/seagate_expans/processed_images/annotations.csv'\n",
        "                             ):\n",
        "    ls = []\n",
        "    ls_files = os.listdir(PATH_IMAGES)\n",
        "\n",
        "    sample_images = random.sample(ls_files, batch_size)\n",
        "    sample_images = list(map(replace_incorrect_names_val, sample_images))\n",
        "    #print(sample_images)\n",
        "    # print(sample_images)\n",
        "    for img_name in sample_images:\n",
        "        #print(img_name[:-4])\n",
        "        #print(PATH_IMAGES + '/'+str(img_name)+'.png')\n",
        "        img=cv2.imread(PATH_IMAGES + '/'+str(img_name))\n",
        "        ls.append(img)\n",
        "        #print(ls)\n",
        "\n",
        "    df = pd.read_csv(PATH_LABELS, header=0, index_col=0)\n",
        "\n",
        "\n",
        "    sample_images = [element[:-4] for element in sample_images]\n",
        "\n",
        "    y = df[df.ix.isin(sample_images)]['mask_condition'].apply(lambda x: 0 if str(x)=='with_mask' else (1 if str(x)=='mask_weared_incorrect' else 2)).to_numpy()\n",
        "\n",
        "    #print(y)\n",
        "    #print(np.array(ls, dtype='float64').shape)\n",
        "    #print(y.shape)\n",
        "    # print(y)\n",
        "    X = np.array(ls, dtype='float64').reshape((batch_size, 80, 80, 3))\n",
        "    return X, y\n",
        "\n",
        "# strategy 1 = train the network with no pretrained parameters,\n",
        "# train the network only with real + classical augmented data\n",
        "def train_network_strategy_1(n_epochs=120, n_batch_update = 128, batch_size=64, val_batch_size=407, save_freq_param=50, PATH_VAL_IMGS = '/Volumes/seagate_expans/val_classical_augm_images/images'):\n",
        "    # model instantiation\n",
        "    base_model = EfficientNetV2S(include_top=True,\n",
        "                            weights=None,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=(80, 80, 3),\n",
        "                            pooling=None,\n",
        "                            classes=3,\n",
        "                            classifier_activation=\"softmax\",\n",
        "                            include_preprocessing=True)\n",
        "\n",
        "    #model = Sequential()\n",
        "    #for layer in base_model.layers[:-1]:\n",
        "    #    model.add(layer)\n",
        "    #model.add(Dense(1, activation='softmax'))\n",
        "    model2 = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "    dense = Dense(1000, activation='relu')(model2.output)\n",
        "    output = Dense(3, activation='softmax')(dense)\n",
        "\n",
        "    tot_model = Model(inputs=base_model.input, outputs=output)\n",
        "\n",
        "    # devo togliere l'ultimo layer e aggiungere un Dense(1)\n",
        "\n",
        "    tot_model.summary()\n",
        "\n",
        "    # define optimizer &  scheduler\n",
        "\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=0.01,\n",
        "    decay_steps=1000,\n",
        "    decay_rate=0.9)\n",
        "    opt = Adam(learning_rate=lr_schedule)\n",
        "\n",
        "    # define a loss function\n",
        "    loss_function = keras.losses.SparseCategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "    w_m = generate_weight_matrix()\n",
        "    #loss_weighted_classes = weighted_categorical_crossentropy(weights=w_m)\n",
        "\n",
        "    # training and validation metric\n",
        "    training_metric = keras.metrics.AUC()\n",
        "    val_metric = keras.metrics.AUC()\n",
        "\n",
        "    # model compilation\n",
        "    #tot_model.compile(loss=loss_function, optimizer=opt, metrics=['AUC'])\n",
        "\n",
        "\n",
        "    for n in range(n_epochs):\n",
        "        for b in range(n_batch_update):\n",
        "            print('Epoch: {}, step: {}'.format(n, b))\n",
        "\n",
        "            # generate training batches\n",
        "\n",
        "            X_batch_train, y_batch_train = generate_training_batch(batch_size)\n",
        "            X_batch_train = preprocess_input(X_batch_train)\n",
        "\n",
        "\n",
        "            # generate validation batches\n",
        "            X_batch_val, y_batch_val = generate_validation_batch(val_batch_size)\n",
        "            X_batch_val = preprocess_input(X_batch_val)\n",
        "\n",
        "\n",
        "\n",
        "            # generate one hot encoded y variables\n",
        "            y_train_one_hot = np.array(pd.Series(y_batch_train).apply(\n",
        "                lambda x: [1, 0, 0] if x == 0 else ([0, 1, 0] if x == 1 else [0, 0, 1])).values.tolist()).reshape(\n",
        "                (batch_size, 3))\n",
        "            y_val_one_hot = np.array(pd.Series(y_batch_val).apply(\n",
        "                lambda x: [1, 0, 0] if x == 0 else ([0, 1, 0] if x == 1 else [0, 0, 1])).values.tolist()).reshape(\n",
        "                (val_batch_size, 3))\n",
        "\n",
        "            # batch updates\n",
        "            # tot_model.train_on_batch(X_batch_train, y_batch_train)\n",
        "            with tf.GradientTape() as tape:\n",
        "\n",
        "                logits = tot_model(X_batch_train, training=True)\n",
        "                #print(y_batch_train)\n",
        "                #print(logits)\n",
        "                lgts = logits.numpy().flatten().reshape((batch_size, 3))\n",
        "                loss_value = loss_function(y_batch_train.reshape((batch_size,1)),logits )\n",
        "                #loss_value = weighted_categorical_crossentropy(y_batch_train.reshape((batch_size,1)),logits, w_m )\n",
        "                print(loss_function(y_batch_train.reshape((batch_size,1)),lgts ))\n",
        "                #quit()\n",
        "            grads = tape.gradient(loss_value, tot_model.trainable_weights)\n",
        "            opt.apply_gradients(zip(grads, tot_model.trainable_weights))\n",
        "\n",
        "            # update training metric\n",
        "            training_metric.update_state(y_train_one_hot, logits)\n",
        "\n",
        "\n",
        "\n",
        "            #print(training_metric)\n",
        "\n",
        "            # Log every 200 batches.\n",
        "            if b % 10 == 0:\n",
        "                print(\n",
        "                    \"Training loss (for one batch) at step %d: %.5f\"\n",
        "                    % (b, float(loss_value))\n",
        "                )\n",
        "                print(\"Seen so far: %d samples\" % ((b + 1) * batch_size))\n",
        "\n",
        "        # Display metrics at the end of each epoch.\n",
        "        train_value = training_metric.result()\n",
        "        print(\"Training AUC over epoch: %.4f\" % (float(train_value),))\n",
        "\n",
        "        # Reset training metrics at the end of each epoch\n",
        "        training_metric.reset_states()\n",
        "        # if n_epochs % save_param == 0:\n",
        "\n",
        "\n",
        "        val_logits = tot_model(X_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_metric.update_state(y_val_one_hot, val_logits)\n",
        "        val_metric_value = val_metric.result()\n",
        "        val_metric.reset_states()\n",
        "        print(\"Validation AUC: %.4f\" % (float(val_metric_value),))\n",
        "        #print(\"Time taken: %.2fs\" % (time.time() - start_time))\n",
        "\n",
        "        # tot model saved weights\n",
        "        if n % save_freq_param == 0:\n",
        "            tot_model.save_weights('models/effectivenet_epoch_'+str(n))\n",
        "\n",
        "\n",
        "def freq_class(int_class, PATH_TO_ANNOTATIONS = '/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv'):\n",
        "    df = pd.read_csv(PATH_TO_ANNOTATIONS, header=0, index_col=0)\n",
        "    class_count = df[df['mask_condition'].astype('int32')==int_class].count().values[0]\n",
        "    #print(class_count)\n",
        "    tot_count = len(df.index)\n",
        "    #print(tot_count)\n",
        "    return class_count/tot_count\n",
        "\n",
        "\n",
        "\n",
        "def train_evaluate_network_strategy_1(training_set_size = 19000, n_epochs=350,\n",
        "                                      n_batch_update = 128,\n",
        "                                      batch_size=32,\n",
        "                                      val_ratio=0.1,\n",
        "                                      save_freq_param=50, class_weights={0:1/freq_class(0),1:1/freq_class(1), 2:1/freq_class(2)}, PATH_VAL_IMGS = '/Volumes/seagate_expans/val_classical_augm_images/images'):\n",
        "    # model instantiation\n",
        "    base_model = EfficientNetV2S(include_top=True,\n",
        "                            weights=None,\n",
        "                            input_tensor=None,\n",
        "                            input_shape=(80, 80, 3),\n",
        "                            pooling=None,\n",
        "                            classes=3,\n",
        "                            classifier_activation=\"softmax\",\n",
        "                            include_preprocessing=True)\n",
        "\n",
        "    #model = Sequential()\n",
        "    #for layer in base_model.layers[:-1]:\n",
        "    #    model.add(layer)\n",
        "    #model.add(Dense(1, activation='softmax'))\n",
        "\n",
        "\n",
        "    #model2 = Model(inputs=base_model.input, outputs=base_model.layers[-2].output)\n",
        "    #dense = Dense(1000, activation='relu')(model2.output)\n",
        "    #output = Dense(3, activation='sigmoid')(dense)\n",
        "    #tot_model = Model(inputs=base_model.input, outputs=output)\n",
        "    tot_model=base_model\n",
        "    tot_model.summary()\n",
        "\n",
        "    # define optimizer &  scheduler\n",
        "    lr_schedule = ExponentialDecay(initial_learning_rate=0.256,\n",
        "    decay_steps=3,\n",
        "    decay_rate=0.965)\n",
        "    opt = Adam(learning_rate=lr_schedule)\n",
        "    opt2 = RMSprop(learning_rate=lr_schedule, momentum=0.9)\n",
        "    # define a loss function\n",
        "    loss_function = keras.losses.CategoricalCrossentropy(from_logits=False)\n",
        "\n",
        "    # callback for early stoppage\n",
        "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "\n",
        "    # model compilation\n",
        "    tot_model.compile(loss=loss_function, optimizer=opt, metrics=[tf.keras.metrics.AUC()])\n",
        "\n",
        "    # load training validation data\n",
        "    X, y = generate_training_data(training_set_size)\n",
        "    X = preprocess_input(X)\n",
        "\n",
        "    print(X.shape)\n",
        "    print(y.shape)\n",
        "    y_train_one_hot = np.array(pd.Series(y).apply(\n",
        "        lambda x: [1, 0, 0] if x == 0 else ([0, 1, 0] if x == 1 else [0, 0, 1])).values.tolist()).reshape(\n",
        "        (training_set_size, 3))\n",
        "\n",
        "    training_metrics = tot_model.fit(x = X, y = y_train_one_hot,\n",
        "                  epochs=n_epochs,\n",
        "                  batch_size=batch_size,\n",
        "                  steps_per_epoch=n_batch_update,\n",
        "                  validation_split=val_ratio,\n",
        "                  class_weight=class_weights,\n",
        "                  #class_weight=None,\n",
        "                  shuffle=True, # shuffles after selecting validation data\n",
        "                  callbacks=[callback],\n",
        "                  workers=-1,\n",
        "                  use_multiprocessing=True,\n",
        "                  verbose=2)\n",
        "\n",
        "    # loading test data\n",
        "    X_test, y_test = generate_training_data(training_set_size)\n",
        "    X_test = preprocess_input(X_test)\n",
        "\n",
        "    y_test_one_hot = np.array(pd.Series(y_test).apply(\n",
        "        lambda x: [1, 0, 0] if x == 0 else ([0, 1, 0] if x == 1 else [0, 0, 1])).values.tolist()).reshape(\n",
        "        (training_set_size, 3))\n",
        "\n",
        "    test_metrics = tot_model.evaluate(x=X_test, y=y_test_one_hot, steps=3)\n",
        "    return training_metrics, test_metrics, tot_model\n",
        "\n",
        "\n",
        "        # tot model saved weights\n",
        "        #if n % save_freq_param == 0:\n",
        "        #    tot_model.save_weights('models/effectivenet_epoch_'+str(n))\n",
        "\n",
        "# return ls of f1 scores\n",
        "def calculate_element_wise_f1_score(ls_prec, ls_recall):\n",
        "    prec=np.array(ls_prec)\n",
        "    rec=np.array(ls_recall)\n",
        "    return 2*prec*rec/(prec+rec)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 552
        },
        "id": "g8mM00q1pu8i",
        "outputId": "098aa616-bdf0-402f-90e2-a4a2d85119de"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-ccc4cc49fc4f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    378\u001b[0m                                       \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                                       \u001b[0mval_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                                       save_freq_param=50, class_weights={0:1/freq_class(0),1:1/freq_class(1), 2:1/freq_class(2)}, PATH_VAL_IMGS = '/Volumes/seagate_expans/val_classical_augm_images/images'):\n\u001b[0m\u001b[1;32m    381\u001b[0m     \u001b[0;31m# model instantiation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m     base_model = EfficientNetV2S(include_top=True,\n",
            "\u001b[0;32m<ipython-input-13-ccc4cc49fc4f>\u001b[0m in \u001b[0;36mfreq_class\u001b[0;34m(int_class, PATH_TO_ANNOTATIONS)\u001b[0m\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfreq_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPATH_TO_ANNOTATIONS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 367\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPATH_TO_ANNOTATIONS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m     \u001b[0mclass_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask_condition'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'int32'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mint_class\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m     \u001b[0;31m#print(class_count)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Volumes/seagate_expans/classical_augm_images/annotations/annotations.csv'"
          ]
        }
      ]
    }
  ]
}